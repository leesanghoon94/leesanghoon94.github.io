--apiserver-advertise-address string The IP address the API Server will advertise it's listening on. If not set the default network interface will be used.

--apiserver-advertise-address string API 서버가 수신 대기 중임을 알리는 IP 주소입니다. 설정하지 않으면 기본 네트워크 인터페이스가 사용됩니다.

--pod-network-cidr string Specify range of IP addresses for the pod network. If set, the control plane will automatically allocate CIDRs for every node.

--pod-network-cidr string 포드 네트워크의 IP 주소 범위를 지정합니다. 설정된 경우 제어 평면은 모든 노드에 CIDR을 자동으로 할당합니다.

I0429 07:57:11.541708 4363 checks.go:327] validating the contents of file /proc/sys/net/ipv4/ip_forward
[preflight] Some fatal errors occurred:
[ERROR CRI]: container runtime is not running: output: time="2024-04-29T07:57:11Z" level=fatal msg="validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`

journalctl -xeu kubelet
░░ The job identifier is 92074 and the job result is done.
Apr 29 10:31:12 control systemd[1]: Started kubelet: The Kubernetes Node Agent.
░░ Subject: A start job for unit kubelet.service has finished successfully
░░ Defined-By: systemd
░░ Support: http://www.ubuntu.com/support
░░
░░ A start job for unit kubelet.service has finished successfully.
░░
░░ The job identifier is 92074.
Apr 29 10:31:12 control kubelet[38545]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kuber>
Apr 29 10:31:12 control kubelet[38545]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
Apr 29 10:31:12 control kubelet[38545]: I0429 10:31:12.670389 38545 server.go:205] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in th>
Apr 29 10:31:12 control kubelet[38545]: E0429 10:31:12.676341 38545 run.go:74] "command failed" err="failed to construct kubelet dependencies: unable to load client CA file /etc/kubernetes/pki/ca.crt: >
Apr 29 10:31:12 control systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
░░ Subject: Unit process exited
░░ Defined-By: systemd
░░ Support: http://www.ubuntu.com/support
░░
░░ An ExecStart= process belonging to unit kubelet.service has exited.
░░
░░ The process' exit code is 'exited' and its exit status is 1.
Apr 29 10:31:12 control systemd[1]: kubelet.service: Failed with result 'exit-code'.
░░ Subject: Unit failed
░░ Defined-By: systemd
░░ Support: http://www.ubuntu.com/support
░░
░░ The unit kubelet.service has entered the 'failed' state with result 'exit-code'.

error retrieving resource lock kube-system/kube-controller-manager: Get "https://10.0.2.15:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s": net/http: request canceled (Client.Timeout exceeded while awaiting headers)

Unable to remove endpoints from kubernetes service: StorageError: key not found, Code: 1, Key: /registry/masterleases/10.0.2.15, ResourceVersion: 0, AdditionalErrorMsg:

```
vagrant ssh worker
vagrant@worker:~$ kubeadm join 192.168.219.170:6443 --token vb1grz.o3c5jlzgrow0tqg0 \
       --discovery-token-ca-cert-hash sha256:6956a1f6ed1ad4e70553609cd864874ff36e08750a456240f39a11c2d6ccbd82
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR IsPrivilegedUser]: user is not running as root
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
vagrant@worker:~$ kubeadm join 192.168.219.170:6443 --token vb1grz.o3c5jlzgrow0tqg0     --discovery-token-ca-cert-hash sha256:6956a1f6ed1ad4e70553609cd864874ff36e08750a456240f39a11c2d6ccbd82 --v=5
I0430 02:49:00.139566    5196 join.go:417] [preflight] found NodeName empty; using OS hostname as NodeName
I0430 02:49:00.139984    5196 initconfiguration.go:122] detected and using CRI socket: unix:///var/run/containerd/containerd.sock
[preflight] Running pre-flight checks
I0430 02:49:00.140283    5196 preflight.go:93] [preflight] Running general checks
[preflight] Some fatal errors occurred:
	[ERROR IsPrivilegedUser]: user is not running as root
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
error execution phase preflight
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:260
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:446
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:232
k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdJoin.func1
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/join.go:183
github.com/spf13/cobra.(*Command).execute
	github.com/spf13/cobra@v1.7.0/command.go:940
github.com/spf13/cobra.(*Command).ExecuteC
	github.com/spf13/cobra@v1.7.0/command.go:1068
github.com/spf13/cobra.(*Command).Execute
	github.com/spf13/cobra@v1.7.0/command.go:992
k8s.io/kubernetes/cmd/kubeadm/app.Run
	k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:52
main.main
	k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25
runtime.main
	runtime/proc.go:271
runtime.goexit
	runtime/asm_amd64.s:1695
vagrant@worker:~$ sudo kubeadm join 192.168.219.170:6443 --token vb1grz.o3c5jlzgrow0tqg0        --discovery-token-ca-cert-hash sha256:6956a1f6ed1ad4e70553609cd864874ff36e08750a456240f39a11c2d6ccbd82 --v=5I0430 02:49:06.101417    5202 join.go:417] [preflight] found NodeName empty; using OS hostname as NodeName
I0430 02:49:06.103766    5202 initconfiguration.go:122] detected and using CRI socket: unix:///var/run/containerd/containerd.sock
[preflight] Running pre-flight checks
I0430 02:49:06.103949    5202 preflight.go:93] [preflight] Running general checks
I0430 02:49:06.104115    5202 checks.go:278] validating the existence of file /etc/kubernetes/kubelet.conf
I0430 02:49:06.104141    5202 checks.go:278] validating the existence of file /etc/kubernetes/bootstrap-kubelet.conf
I0430 02:49:06.104151    5202 checks.go:102] validating the container runtime
I0430 02:49:06.182393    5202 checks.go:637] validating whether swap is enabled or not
	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0430 02:49:06.182707    5202 checks.go:368] validating the presence of executable crictl
I0430 02:49:06.182758    5202 checks.go:368] validating the presence of executable conntrack
I0430 02:49:06.182799    5202 checks.go:368] validating the presence of executable ip
I0430 02:49:06.183143    5202 checks.go:368] validating the presence of executable iptables
I0430 02:49:06.183423    5202 checks.go:368] validating the presence of executable mount
I0430 02:49:06.183489    5202 checks.go:368] validating the presence of executable nsenter
I0430 02:49:06.183716    5202 checks.go:368] validating the presence of executable ebtables
I0430 02:49:06.183859    5202 checks.go:368] validating the presence of executable ethtool
I0430 02:49:06.184013    5202 checks.go:368] validating the presence of executable socat
I0430 02:49:06.184304    5202 checks.go:368] validating the presence of executable tc
I0430 02:49:06.184416    5202 checks.go:368] validating the presence of executable touch
I0430 02:49:06.184622    5202 checks.go:514] running all checks
I0430 02:49:06.361927    5202 checks.go:399] checking whether the given node name is valid and reachable using net.LookupHost
I0430 02:49:06.364061    5202 checks.go:603] validating kubelet version
I0430 02:49:06.629892    5202 checks.go:128] validating if the "kubelet" service is enabled and active
I0430 02:49:06.656427    5202 checks.go:201] validating availability of port 10250
I0430 02:49:06.657251    5202 checks.go:278] validating the existence of file /etc/kubernetes/pki/ca.crt
I0430 02:49:06.657357    5202 checks.go:428] validating if the connectivity type is via proxy or direct
I0430 02:49:06.657583    5202 checks.go:327] validating the contents of file /proc/sys/net/ipv4/ip_forward
I0430 02:49:06.657869    5202 join.go:536] [preflight] Discovering cluster-info
I0430 02:49:06.658115    5202 token.go:79] [discovery] Created cluster-info discovery client, requesting info from "192.168.219.170:6443"
I0430 02:49:06.662664    5202 token.go:210] [discovery] Waiting for the cluster-info ConfigMap to receive a JWS signaturefor token ID "vb1grz"
I0430 02:49:06.699205    5202 token.go:228] [discovery] Retrying due to error: could not find a JWS signature in the cluster-info ConfigMap for token ID "vb1grz"
I0430 02:49:11.686361    5202 token.go:117] [discovery] Requesting info from "192.168.219.170:6443" again to validate TLS against the pinned public key
I0430 02:49:11.687158    5202 token.go:210] [discovery] Waiting for the cluster-info ConfigMap to receive a JWS signaturefor token ID "vb1grz"
I0430 02:49:11.701739    5202 token.go:134] [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "192.168.219.170:6443"
I0430 02:49:11.702324    5202 discovery.go:52] [discovery] Using provided TLSBootstrapToken as authentication credentials for the join process
I0430 02:49:11.702340    5202 join.go:550] [preflight] Fetching init configuration
I0430 02:49:11.702432    5202 join.go:596] [preflight] Retrieving KubeConfig objects
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
I0430 02:49:11.718042    5202 kubeproxy.go:55] attempting to download the KubeProxyConfiguration from ConfigMap "kube-proxy"
I0430 02:49:11.728375    5202 kubelet.go:74] attempting to download the KubeletConfiguration from ConfigMap "kubelet-config"
I0430 02:49:11.740481    5202 initconfiguration.go:114] skip CRI socket detection, fill with the default CRI socket unix:///var/run/containerd/containerd.sock
I0430 02:49:11.743348    5202 interface.go:432] Looking for default routes with IPv4 addresses
I0430 02:49:11.743412    5202 interface.go:437] Default route transits interface "eth0"
I0430 02:49:11.743572    5202 interface.go:209] Interface eth0 is up
I0430 02:49:11.743668    5202 interface.go:257] Interface "eth0" has 2 addresses :[10.0.2.15/24 fe80::a00:27ff:fe06:e856/64].
I0430 02:49:11.743701    5202 interface.go:224] Checking addr  10.0.2.15/24.
I0430 02:49:11.743718    5202 interface.go:231] IP found 10.0.2.15
I0430 02:49:11.743803    5202 interface.go:263] Found valid IPv4 address 10.0.2.15 for interface "eth0".
I0430 02:49:11.743833    5202 interface.go:443] Found active IP 10.0.2.15
I0430 02:49:11.751002    5202 preflight.go:104] [preflight] Running configuration dependant checks
I0430 02:49:11.751590    5202 controlplaneprepare.go:225] [download-certs] Skipping certs download
I0430 02:49:11.751682    5202 kubelet.go:122] [kubelet-start] writing bootstrap kubelet config file at /etc/kubernetes/bootstrap-kubelet.conf
I0430 02:49:11.753147    5202 kubelet.go:137] [kubelet-start] writing CA certificate at /etc/kubernetes/pki/ca.crt
I0430 02:49:11.754210    5202 kubelet.go:158] [kubelet-start] Checking for an existing Node in the cluster with name "worker" and status "Ready"
I0430 02:49:11.759452    5202 kubelet.go:173] [kubelet-start] Stopping the kubelet
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s



[kubelet-check] The kubelet is not healthy after 4m0.00009456s

Unfortunately, an error has occurred:
	The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' returned error: Get "http://localhost:10248/healthz": dial tcp 127.0.0.1:10248: connect: connection refused


This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'
The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' returned error: Get "http://localhost:10248/healthz": dial tcp 127.0.0.1:10248: connect: connection refused

k8s.io/kubernetes/cmd/kubeadm/app/util/apiclient.(*KubeWaiter).WaitForKubelet.func1
	k8s.io/kubernetes/cmd/kubeadm/app/util/apiclient/wait.go:256
k8s.io/kubernetes/cmd/kubeadm/app/util/apiclient.(*KubeWaiter).WaitForKubelet.func2
	k8s.io/kubernetes/cmd/kubeadm/app/util/apiclient/wait.go:273
k8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext.func2
	k8s.io/apimachinery/pkg/util/wait/loop.go:87
k8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext
	k8s.io/apimachinery/pkg/util/wait/loop.go:88
k8s.io/apimachinery/pkg/util/wait.PollUntilContextTimeout
	k8s.io/apimachinery/pkg/util/wait/poll.go:48
k8s.io/kubernetes/cmd/kubeadm/app/util/apiclient.(*KubeWaiter).WaitForKubelet
	k8s.io/kubernetes/cmd/kubeadm/app/util/apiclient/wait.go:260
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/join.runKubeletStartJoinPhase
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/join/kubelet.go:211
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:259
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:446
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:232
k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdJoin.func1
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/join.go:183
github.com/spf13/cobra.(*Command).execute
	github.com/spf13/cobra@v1.7.0/command.go:940
github.com/spf13/cobra.(*Command).ExecuteC
	github.com/spf13/cobra@v1.7.0/command.go:1068
github.com/spf13/cobra.(*Command).Execute
	github.com/spf13/cobra@v1.7.0/command.go:992
k8s.io/kubernetes/cmd/kubeadm/app.Run
	k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:52
main.main
	k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25
runtime.main
	runtime/proc.go:271
runtime.goexit
	runtime/asm_amd64.s:1695
error execution phase kubelet-start
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run.func1
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:260
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).visitAll
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:446
k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow.(*Runner).Run
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow/runner.go:232
k8s.io/kubernetes/cmd/kubeadm/app/cmd.newCmdJoin.func1
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/join.go:183
github.com/spf13/cobra.(*Command).execute
	github.com/spf13/cobra@v1.7.0/command.go:940
github.com/spf13/cobra.(*Command).ExecuteC
	github.com/spf13/cobra@v1.7.0/command.go:1068
github.com/spf13/cobra.(*Command).Execute
	github.com/spf13/cobra@v1.7.0/command.go:992
k8s.io/kubernetes/cmd/kubeadm/app.Run
	k8s.io/kubernetes/cmd/kubeadm/app/kubeadm.go:52
main.main
	k8s.io/kubernetes/cmd/kubeadm/kubeadm.go:25
runtime.main
	runtime/proc.go:271
runtime.goexit
	runtime/asm_amd64.s:1695
vagrant@worker:~$
```

You will then be prompted to login as:. Here you will enter the username you created previously.

If using Linux or OS X:

Linux/Mac: ssh -l _username_ -p 2222 127.0.0.1
... remember to replace _username_ with the user you created!

You will be prompted to accept the host key:
https://nsrc.org/workshops/2014/sanog23-virtualization/raw-attachment/wiki/Agenda/ex-virtualbox-portforward-ssh.htm

다음은 사용자의 그룹을 변경하여 "sex" 그룹에서 "control" 그룹으로 변경하는 방법입니다:

bash
Copy code
sudo usermod -g control sex

사용자를 삭제하기 위해선 userdel 명령어를 사용합니다. 다음 명령어를 사용하여 "sex" 사용자를 삭제할 수 있습니다:

bash
Copy code
sudo userdel sex
이 명령어를 실행하면 "sex" 사용자가 시스템에서 삭제됩니다. 그러나 이렇게 하면 해당 사용자의 홈 디렉토리나 관련 파일이나 디렉토리는 삭제되지 않습니다. 만약 사용자의 홈 디렉토리와 관련 파일을 모두 삭제하려면 -r 옵션을 추가하여 실행할 수 있습니다:

lsof -i -P -n | grep LISTEN
rapportd 434 lee 3u IPv4 0xee3df7a80fa33c7d 0t0 TCP _:65454 (LISTEN)
rapportd 434 lee 4u IPv6 0xee3df7a80ef9d815 0t0 TCP _:65454 (LISTEN)
mysqld 750 lee 18u IPv4 0xee3df7a80fa568fd 0t0 TCP 127.0.0.1:33060 (LISTEN)
mysqld 750 lee 20u IPv4 0xee3df7a80fa552bd 0t0 TCP 127.0.0.1:3306 (LISTEN)

Identify the cgroup version on Linux Nodes
The cgroup version depends on the Linux distribution being used and the default cgroup version configured on the OS. To check which cgroup version your distribution uses, run the stat -fc %T /sys/fs/cgroup/ command on the node:

stat -fc %T /sys/fs/cgroup/
For cgroup v2, the output is cgroup2fs.

For cgroup v1, the output is tmpfs.
